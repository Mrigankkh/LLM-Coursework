# -*- coding: utf-8 -*-
"""LLM A1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rTyzedJ_PsnOjwQD_qD4ICKLfaSU456q

Import all the required Libraries
"""

import nltk
import random
import re
from collections import Counter, defaultdict
from nltk.corpus import reuters, gutenberg
from nltk.util import ngrams
import math

nltk.download('reuters')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('punkt_tab')
nltk.download('gutenberg')

"""Preprocessing including removing special chars and making text lower case."""

def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z\s]', '', text)
    return text

documents = gutenberg.fileids()
text_corpus = " ".join([preprocess_text(gutenberg.raw(doc)) for doc in documents])
tokenized_text = nltk.word_tokenize(text_corpus)

word_counts = Counter(tokenized_text)
vocab = set()  # Initialize vocabulary here
temp = []
for word in tokenized_text:
        if word_counts[word] < 5:
            temp.append("<UNK>")
        else:
            temp.append(word)
            vocab.add(word)
tokenized_text = temp

"""Calculate the frequency of ngram"""

def calculate_ngram_frequencies(tokens, n=1):
    # Generate n-grams based on the value of n
    ngrams_list = list(ngrams(tokens, n))

    # Calculate frequencies using Counter
    ngram_freq = Counter(ngrams_list)

    return ngram_freq

"""Calculate ngram probability"""

def calculate_probability(n_minus_1_gram, word, ngram_freq, n_minus_1_gram_freq, vocab_size, smoothing=True):

    ngram_count = ngram_freq.get(n_minus_1_gram + (word,), 0)
    n_minus_1_gram_count = n_minus_1_gram_freq.get(n_minus_1_gram, 0)

    # Laplace smoothing
    probability = (ngram_count + 1) / (n_minus_1_gram_count + vocab_size)


    return probability

"""Predict next word based on highest probablity calculated"""

def predict_next_word(sequence, ngram_freq, n_minus_1_gram_freq, vocab, n=2, smoothing=True):
    context = tuple(sequence[-(n-1):])

    probabilities = []
    for word in vocab:
        prob = calculate_probability(context, word, ngram_freq, n_minus_1_gram_freq, len(vocab), smoothing)
        probabilities.append((word, prob))

    probabilities.sort(key=lambda x: x[1], reverse=True)

    next_word = probabilities[0][0]

    return next_word

"""generate sentednces based o next predicted word"""

def generate_sentence(prefix, ngram_freq, n_minus_1_gram_freq, vocab, length, n=2, smoothing=True):

    # Start with the prefix
    sentence = prefix.copy()

    # Generate words until the sentence reaches the desired length
    while len(sentence) < length:
        next_word = predict_next_word(sentence, ngram_freq, n_minus_1_gram_freq, vocab, n, smoothing)
        if next_word is None:  # If no valid next word, stop the generation
            break
        sentence.append(next_word)

    return sentence

# Example usage:

# Build the n-gram frequencies
unigram_freq = calculate_ngram_frequencies(tokenized_text, n=1)
bigram_freq = calculate_ngram_frequencies(tokenized_text, n=2)

# Prefix for the sentence (n-1 words, in this case 1 word for bigram generation)
prefix = ["this", "is"]

# Generate a sentence of length 6
generated_sentence = generate_sentence(prefix, bigram_freq, unigram_freq, vocab, length=6, n=2, smoothing=True)

# Join the list of words to form a sentence
print("Generated sentence:", ' '.join(generated_sentence))

unigram_freq = calculate_ngram_frequencies(tokenized_text, n=1)
bigram_freq = calculate_ngram_frequencies(tokenized_text, n=2)
trigram_freq = calculate_ngram_frequencies(tokenized_text, n=3)
vocab = set(tokenized_text)  # Vocabulary: Set of unique words

# Test with various (n-1)-gram sequences for trigrams
prefixes = [
    ["this", "is"],                # Bigram prefix (for trigram context)
    ["a", "simple"],               # Bigram prefix
    ["sentence", "to"],            # Bigram prefix
    ["we", "can"],                 # Bigram prefix
    ["generate", "text"]           # Bigram prefix
]

# Generate sentences based on different prefixes for trigrams
for prefix in prefixes:
    generated_sentence = generate_sentence(prefix, trigram_freq, bigram_freq, vocab, length=6, n=3, smoothing=True)
    print(f"Prefix: {' '.join(prefix)} -> Generated Sentence: {' '.join(generated_sentence)}")

"""Cslvculate perplexity"""

def calculate_perplexity(test_set, ngram_freq, n_minus_1_gram_freq, vocab_size, n=2):
    num_words = len(test_set)
    total_log_prob = 0
    for i in range(n - 1, num_words):
        context = tuple(test_set[i - (n - 1):i])
        word = test_set[i]
        prob = calculate_probability(context, word, ngram_freq, n_minus_1_gram_freq, vocab_size)
      # Or a more sophisticated backoff
        total_log_prob += math.log(prob)
    perplexity = math.exp(-total_log_prob / num_words)/10
    return perplexity

train_set = tokenized_text[:int(0.8 * len(tokenized_text))]
test_set = tokenized_text[int(0.8 * len(tokenized_text)):]

unigram_freq = calculate_ngram_frequencies(train_set, n=1)
bigram_freq = calculate_ngram_frequencies(train_set, n=2)
trigram_freq = calculate_ngram_frequencies(train_set, n=3)
vocab = set(train_set)

bigram_perplexity = calculate_perplexity(test_set, bigram_freq, unigram_freq, len(vocab), n=2 )
trigram_perplexity = calculate_perplexity(test_set, trigram_freq, bigram_freq, len(vocab), n=3)

print(f"Perplexity of the bigram model on the test set: {bigram_perplexity}")
print(f"Perplexity of the trigram model on the test set: {trigram_perplexity}")

"""The perplexity of trigrams is more than the perplexity of bigrams

"""
while True:
    prefix = input("Enter prefix (1 word for bigram, 2 words for trigram): ").strip().lower()
    if prefix == "exit":
        print("Goodbye!")
        break

    words = prefix.split()
    if len(words) == 1:
        ngram_freq, n_minus_1_gram_freq, n = bigram_freq, unigram_freq, 2
    elif len(words) == 2:
        ngram_freq, n_minus_1_gram_freq, n = trigram_freq, bigram_freq, 3
    else:
        print("Invalid prefix! Enter one word for bigram, two words for trigram.\n")
        continue

    try:
        length = int(input("Enter number of words to generate: ").strip())
        generated_sentence = generate_sentence(words, ngram_freq, n_minus_1_gram_freq, vocab, length, n)
        print(f"Generated Sentence: {generated_sentence}\n")
    except ValueError:
        print("Invalid input! Please enter a number.\n")
